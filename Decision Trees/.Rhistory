#loading library and attaching it.
library(ISLR)
attach(Default)
#Some functions to better understand the data.
dim(Default)
names(Default)
# one of many ways students may divide into trainingSet/testSet
set.seed(2017)
len_df <- nrow(Default)
train <- sample(seq_len(len_df), size=floor(0.8*len_df))
trainingSet <- Default[train,]
testSet <- Default[-train,]
df <- read.csv("data/heart.csv", header=TRUE)
plot(tree1)
logisticModel = glm(default~., data = trainingSet, family = binomial)
summary(logisticModel)
pred <- predict(logisticModel, newdata=testSet, type="response")
pred <- ifelse(pred>.5, "Yes", "No")
table(pred, testSet$default)
mean(pred==testSet$default)
library(tree)
library(tree)
install.packages("tree")
library(tree)
tree1 <- tree(default~., trainingSet)
summary(tree1)
tree_pred <- predict(tree1, testSet, type="class")
table(tree_pred, testSet$default)
mean(tree_pred == testSet$default)
plot(tree1)
text(tree1, pretty=0)
tree1
#Creating a decision tree.
#installing and calling library tree.
install.packages(tree)
library(tree)
#creating a tree model.
decisionTreeModel <- tree(default~., trainingSet)
#loading library and attaching it.
library(ISLR)
attach(Default)
#Some functions to better understand the data.
dim(Default)
names(Default)
set.seed(2017)
dataframeLength <- nrow(Default)
train <- sample(seq_len(dataframeLength), size=floor(0.8 * dataframeLength))
trainingSet <- Default[train,]
testSet <- Default[-train,]
#creating a logistic model.
logisticModel = glm(default~., data = trainingSet, family = binomial)
summary(logisticModel)
#if it is greater than 0.5, then yes otherwise no.
pred <- predict(logisticModel, newdata = testSet, type = "response")
pred <- ifelse(pred > 0.5, "Yes", "No")
#creating a table and then calculating mean.
table(pred, testSet$default)
mean(pred==testSet$default)
#Creating a decision tree.
#installing and calling library tree.
install.packages(tree)
library(tree)
#creating a tree model.
decisionTreeModel <- tree(default~., trainingSet)
summary(decisionTreeModel)
#predictions. Creating table and then predicting mean.
predictions <- predict(decisionTreeModel, testSet, type = "class")
table(predictions, testSet$default)
mean(predictions == testSet$default)
#Creating a plot
plot(decisionTreeModel)
text(decisionTreeModel, pretty = 0)
decisionTreeModel
dataframe <- read.csv("data/heart.csv", header = TRUE)
setwd("C:/Users/Faraz Khalid/Desktop/Faraz Khalid/University/Spring 2018/Machine Learning/Homework/Homework 6")
dataframe <- read.csv("data/heart.csv", header = TRUE)
dataframe <- read.csv("Heart.csv", header = TRUE)
dataframe <- read.csv("Heart.csv", header = TRUE)
attach(dataframe)
#removing the first column that is X.
dataframe <- dataframe[,-1]
#setting seed to 2017 as described in the problem.
set.seed(2017)
dataframeLength <- nrow(dataframe)
train <- sample(seq_len(dataframeLength), size=floor(0.8*dataframeLength))
trainingSet <- dataframe[train,]
testSet <- dataframe[-train,]
#Creating a logistic model.
logisticModel = glm(AHD~., data = trainingSet, family=binomial)
summary(logisticModel)
pred <- predict(logisticModel, newdata=testSet, type="response")
pred <- ifelse(pred>.5, "Yes", "No")
table(pred, testSet$AHD)
#total of correct answers divided by total number of observations.
accuracy = (29+23)/(29+23+3+4)
#printing accuracy on screen.
accuracy
#Creating a decision tree model.
library(tree)
tree2 <- tree(AHD~., trainingSet)
summary(tree2)
tree_pred2 <- predict(tree2, testSet, type = "class")
#creating a table and calculating the mean.
table(tree_pred2, testSet$AHD)
mean(tree_pred2 == testSet$AHD)
plot(tree2)
text(tree2, pretty=0)
tree2
cv.tree2 = cv.tree(tree2, FUN=prune.misclass)
cv.tree2
par(mfrow=c(1,2))
plot(cv.tree2$size, cv.tree2$dev, type="b")
plot(cv.tree2$k, cv.tree2$dev, type="b")
prune.tree2 <- prune.misclass(tree2, best=8)
plot(prune.tree2)
text(prune.tree2, pretty=0)
prune.tree2
prune.tree2.pred <- predict(prune.tree2, testSet, type="class")
table(prune.tree2.pred, testSet$AHD)
mean(prune.tree2.pred==testSet$AHD)
head(Default)
cv.decisionTreeModel2 = cv.tree(decisionTreeModel2, FUN = prune.misclass)
#loading library and attaching it.
library(ISLR)
attach(Default)
#Some functions to better understand the data.
dim(Default)
names(Default)
set.seed(2017)
dataframeLength <- nrow(Default)
train <- sample(seq_len(dataframeLength), size=floor(0.80 * dataframeLength))
trainingSet <- Default[train,]
testSet <- Default[-train,]
#creating a logistic model.
logisticModel = glm(default~., data = trainingSet, family = binomial)
summary(logisticModel)
#if it is greater than 0.5, then yes otherwise no.
pred <- predict(logisticModel, newdata = testSet, type = "response")
pred <- ifelse(pred > 0.5, "Yes", "No")
#creating a table and then calculating mean.
table(pred, testSet$default)
mean(pred==testSet$default)
#Creating a decision tree.
#installing and calling library tree.
install.packages(tree)
library(tree)
#creating a tree model.
decisionTreeModel <- tree(default~., trainingSet)
summary(decisionTreeModel)
#predictions. Creating table and then predicting mean.
predictions <- predict(decisionTreeModel, testSet, type = "class")
table(predictions, testSet$default)
mean(predictions == testSet$default)
#Creating a plot
plot(decisionTreeModel)
text(decisionTreeModel, pretty = 0)
decisionTreeModel
dataframe <- read.csv("Heart.csv", header = TRUE)
attach(dataframe)
#removing the first column that is X.
dataframe <- dataframe[,-1]
#setting seed to 2017 as described in the problem.
set.seed(2017)
dataframeLength <- nrow(dataframe)
train <- sample(seq_len(dataframeLength), size=floor(0.8*dataframeLength))
trainingSet <- dataframe[train,]
testSet <- dataframe[-train,]
#Creating a logistic model.
logisticModel = glm(AHD~., data = trainingSet, family=binomial)
summary(logisticModel)
pred <- predict(logisticModel, newdata=testSet, type="response")
pred <- ifelse(pred>.5, "Yes", "No")
table(pred, testSet$AHD)
#total of correct answers divided by total number of observations.
accuracy = (29+23)/(29+23+3+4)
#printing accuracy on screen.
accuracy
#Creating a decision tree model.
library(tree)
decisionTreeModel2 <- tree(AHD ~ ., trainingSet)
summary(decisionTreeModel2)
tree_pred2 <- predict(decisionTreeModel2, testSet, type = "class")
#creating a table and calculating the mean.
table(tree_pred2, testSet$AHD)
mean(tree_pred2 == testSet$AHD)
plot(decisionTreeModel2)
text(decisionTreeModel2, pretty = 0)
decisionTreeModel2
cv.decisionTreeModel2 = cv.tree(decisionTreeModel2, FUN = prune.misclass)
cv.decisionTreeModel2
par(mfrow=c(1,2))
plot(cv.decisionTreeModel2$size, cv.decisionTreeModel2$dev, type="b")
plot(cv.decisionTreeModel2$k, cv.decisionTreeModel2$dev, type="b")
prune.decisionTreeModel2 <- prune.misclass(decisionTreeModel2, best = 8)
plot(prune.decisionTreeModel2)
text(prune.decisionTreeModel2, pretty = 0)
prune.decisionTreeModel2
prune.decisionTreeModel2.pred <- predict(prune.decisionTreeModel2, testSet, type = "class")
table(prune.decisionTreeModel2.pred, testSet$AHD)
mean(prune.decisionTreeModel2.pred==testSet$AHD)
#loading library and attaching it.
library(ISLR)
attach(Default)
#Some functions to better understand the data.
dim(Default)
names(Default)
set.seed(2017)
dataframeLength <- nrow(Default)
train <- sample(seq_len(dataframeLength), size=floor(0.80 * dataframeLength))
trainingSet <- Default[train,]
testSet <- Default[-train,]
#creating a logistic model.
logisticModel = glm(default~., data = trainingSet, family = binomial)
summary(logisticModel)
#if it is greater than 0.5, then yes otherwise no.
pred <- predict(logisticModel, newdata = testSet, type = "response")
pred <- ifelse(pred > 0.5, "Yes", "No")
#creating a table and then calculating mean.
table(pred, testSet$default)
mean(pred==testSet$default)
#Creating a decision tree.
#installing and calling library tree.
install.packages(tree)
library(tree)
#creating a tree model.
decisionTreeModel <- tree(default~., trainingSet)
summary(decisionTreeModel)
#predictions. Creating table and then predicting mean.
predictions <- predict(decisionTreeModel, testSet, type = "class")
table(predictions, testSet$default)
mean(predictions == testSet$default)
dataframe <- read.csv("Heart.csv", header = TRUE)
attach(dataframe)
#removing the first column that is X.
dataframe <- dataframe[,-1]
#setting seed to 2017 as described in the problem.
set.seed(2017)
dataframeLength <- nrow(dataframe)
train <- sample(seq_len(dataframeLength), size=floor(0.8*dataframeLength))
trainingSet <- dataframe[train,]
testSet <- dataframe[-train,]
#Creating a logistic model.
logisticModel = glm(AHD~., data = trainingSet, family=binomial)
summary(logisticModel)
pred <- predict(logisticModel, newdata=testSet, type="response")
pred <- ifelse(pred > 0.5, "Yes", "No")
table(pred, testSet$AHD)
#total of correct answers divided by total number of observations.
accuracy = (29+23)/(29+23+3+4)
#printing accuracy on screen.
accuracy
mean(pred == testSet$default)
#Creating a logistic model.
logisticModel = glm(AHD~., data = trainingSet, family=binomial)
summary(logisticModel)
pred <- predict(logisticModel, newdata=testSet, type="response")
pred <- ifelse(pred > 0.5, "Yes", "No")
table(pred, testSet$AHD)
#total of correct answers divided by total number of observations.
accuracy = (29 + 23)/(29 + 23 + 3 + 4)
#printing accuracy on screen.
accuracy
#Creating a decision tree model.
library(tree)
decisionTreeModel2 <- tree(AHD ~ ., trainingSet)
summary(decisionTreeModel2)
tree_pred2 <- predict(decisionTreeModel2, testSet, type = "class")
#creating a table and calculating the mean.
table(tree_pred2, testSet$AHD)
mean(tree_pred2 == testSet$AHD)
#setting seed to 2017 as described in the problem.
set.seed(2017)
dataframeLength <- nrow(dataframe)
train <- sample(seq_len(dataframeLength), size=floor(0.8*dataframeLength))
trainingSet <- dataframe[train,]
testSet <- dataframe[-train,]
